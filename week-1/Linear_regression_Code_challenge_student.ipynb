{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176c8155",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "## Integrated Project: Understanding the yield\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this coding challenge, we will apply all of the skills we learned in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "⚠️ **Note that this code challenge is graded and will contribute to your overall marks for this module. Submit this notebook for grading. Note that the names of the functions are different in this notebook. Transfer the code in your notebook to this submission notebook**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- **Do not add or remove cells in this notebook. Do not edit or remove the `### START FUNCTION` or `### END FUNCTION` comments. Do not add any code outside of the functions you are required to edit. Doing any of this will lead to a mark of 0%!**\n",
    "\n",
    "- Answer the questions according to the specifications provided.\n",
    "\n",
    "- Use the given cell in each question to see if your function matches the expected outputs.\n",
    "\n",
    "- Do not hard-code answers to the questions.\n",
    "\n",
    "- The use of StackOverflow, Google, and other online tools is permitted. However, copying a fellow student's code is not permissible and is considered a breach of the Honour code. Doing this will result in a mark of 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ccbc",
   "metadata": {},
   "source": [
    "# Introduction to Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e633f",
   "metadata": {},
   "source": [
    "Simple linear regression is a fundamental statistical method used to quantify the relationship between two variables. It allows us to predict an outcome (dependent variable) based on the value of one predictor (independent variable). In this challenge, we will apply simple linear regression to understand how different environmental factors affect the standardised yield of crops.\n",
    "\n",
    "Our insights will not only help local farmers maximise their harvests but also contribute to the sustainable agriculture practices in Maji Ndogo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837de59",
   "metadata": {},
   "source": [
    "# Initial data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b362",
   "metadata": {},
   "source": [
    "Before we sow the seeds of our regression model, we need to get to know our soil – the dataset. This dataset was developed through extensive agricultural surveys conducted at farms across Maji Ndogo. It contains various factors that might influence a farm's crop yield, from the elevation of the fields to the average temperature they bask in.\n",
    "\n",
    "Spend some time looking at the data dictionary and start thinking about what could be influencing our crop yield."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147be850",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e55e",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield (DUMMY VARIABLE - Removed)\n",
    "\n",
    "<br>\n",
    "\n",
    "**5. Target variable**\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18740c83-966d-4609-8fb0-1d203d7ca8ea",
   "metadata": {},
   "source": [
    "Let's import our database again, like we did last time. We won't use the weather data so it is commented out.\n",
    "\n",
    "**Important:** Ensure that the `data_ingestion.py` file and the `field_data_processor.py` files are stored in the same folder as your notebook, otherwise the data import will fail. The links to the files are below:\n",
    "\n",
    "[Download files here](https://github.com/Explore-AI/Public-Data/raw/master/Maji_Ndogo/modules.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17334162-22e6-4795-bce9-a815b8267bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 05:31:11,714 - data_ingestion - INFO - Database engine created successfully.\n",
      "2025-11-09 05:31:11,716 - data_ingestion - ERROR - An error occurred while querying the database. Error: (sqlite3.OperationalError) no such table: geographic_features\n",
      "[SQL: \n",
      "            SELECT *\n",
      "            FROM geographic_features\n",
      "            LEFT JOIN weather_features USING (Field_ID)\n",
      "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
      "            LEFT JOIN farm_management_features USING (Field_ID)\n",
      "            ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: geographic_features\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[1;32m   1968\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1969\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:951\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 951\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: geographic_features",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Ignoring the field data for now.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m field_processor \u001b[38;5;241m=\u001b[39m FieldDataProcessor(config_params)\n\u001b[0;32m---> 33\u001b[0m field_processor\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[1;32m     34\u001b[0m field_df \u001b[38;5;241m=\u001b[39m field_processor\u001b[38;5;241m.\u001b[39mdf\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# We're not going to use the weather data this time, so we'll ignore it.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# weather_processor = WeatherDataProcessor(config_params)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# weather_processor.process()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# weather_df = weather_processor.weather_df\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/ALX-Machine-Learning-Code-Challenges/week-1/field_data_processor.py:183\u001b[0m, in \u001b[0;36mFieldDataProcessor.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    Applies corrections to the DataFrame based on mappings specified in values_to_rename.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    - The DataFrame with corrections applied to the specified column.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mingest_sql_data()\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrename_columns()\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_corrections()\n",
      "File \u001b[0;32m~/Desktop/projects/ALX-Machine-Learning-Code-Challenges/week-1/field_data_processor.py:110\u001b[0m, in \u001b[0;36mFieldDataProcessor.ingest_sql_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mIngests data from a SQL database based on the configured database path and SQL query.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m- A pandas DataFrame containing the data fetched from the database.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m create_db_engine(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m query_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_query)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSucessfully loaded data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/ALX-Machine-Learning-Code-Challenges/week-1/data_ingestion.py:79\u001b[0m, in \u001b[0;36mquery_data\u001b[0;34m(engine, sql_query)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Desktop/projects/ALX-Machine-Learning-Code-Challenges/week-1/data_ingestion.py:66\u001b[0m, in \u001b[0;36mquery_data\u001b[0;34m(engine, sql_query)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[0;32m---> 66\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql_query(text(sql_query), connection)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# Log a message or handle the empty DataFrame scenario as needed\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/pandas/io/sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[1;32m    527\u001b[0m         sql,\n\u001b[1;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/pandas/io/sql.py:1836\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1781\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1788\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \n\u001b[1;32m   1835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1836\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[1;32m   1837\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/pandas/io/sql.py:1660\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexec_driver_sql(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m-> 1660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1419\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1421\u001b[0m         distilled_parameters,\n\u001b[1;32m   1422\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[1;32m   1423\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:526\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\u001b[38;5;241m.\u001b[39m_execute_clauseelement(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28mself\u001b[39m, distilled_params, execution_options\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1641\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1629\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1630\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1631\u001b[0m )\n\u001b[1;32m   1633\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1634\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1635\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1640\u001b[0m )\n\u001b[0;32m-> 1641\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_context(\n\u001b[1;32m   1642\u001b[0m     dialect,\n\u001b[1;32m   1643\u001b[0m     dialect\u001b[38;5;241m.\u001b[39mexecution_ctx_cls\u001b[38;5;241m.\u001b[39m_init_compiled,\n\u001b[1;32m   1644\u001b[0m     compiled_sql,\n\u001b[1;32m   1645\u001b[0m     distilled_parameters,\n\u001b[1;32m   1646\u001b[0m     execution_options,\n\u001b[1;32m   1647\u001b[0m     compiled_sql,\n\u001b[1;32m   1648\u001b[0m     distilled_parameters,\n\u001b[1;32m   1649\u001b[0m     elem,\n\u001b[1;32m   1650\u001b[0m     extracted_params,\n\u001b[1;32m   1651\u001b[0m     cache_hit\u001b[38;5;241m=\u001b[39mcache_hit,\n\u001b[1;32m   1652\u001b[0m )\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1655\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1656\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1660\u001b[0m         ret,\n\u001b[1;32m   1661\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_single_context(\n\u001b[1;32m   1847\u001b[0m         dialect, context, statement, parameters\n\u001b[1;32m   1848\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(\n\u001b[1;32m   1987\u001b[0m         e, str_statement, effective_parameters, cursor, context\n\u001b[1;32m   1988\u001b[0m     )\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2355\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[1;32m   1968\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1969\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1974\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1979\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dsenv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:951\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 951\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: geographic_features\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Read the database, and clean the data using the processing modules we built.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "# from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "    \"regex_patterns\" : {\n",
    "            'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "            'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "            'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "            },\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# We're not going to use the weather data this time, so we'll ignore it.\n",
    "# weather_processor = WeatherDataProcessor(config_params)\n",
    "# weather_processor.process()\n",
    "# weather_df = weather_processor.weather_df\n",
    "\n",
    "dataset = field_df.drop(\"Weather_station\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73069a-f8e5-43a0-8d87-bc27760a99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a6759-4747-4b03-8efa-7431ffe67488",
   "metadata": {},
   "source": [
    "Before diving into our analysis, it's crucial to ensure the integrity of our dataset and that the data are still as we expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data\n",
    "# !pip install pytest\n",
    "\n",
    "dataset.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324a485",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fddb1",
   "metadata": {},
   "source": [
    "## Challenge 1: Visualising the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8dc1b",
   "metadata": {},
   "source": [
    "With our data ready and loaded, it's time to start exploring.\n",
    "\n",
    "Our goal is to determine whether any of the features in our dataset are influencing the Standard_yield of a farm. If we can figure out what these relationships are, then we can use them to start predicting what future yields will be, based on these features.\n",
    "\n",
    "For this analysis, we want to find whether any features have a linear relationship with Standard_yield so that we can fit a linear regression model to the data. This is important because if we try and fit a linear regression model to non-linear data, our predictions won't be good.\n",
    "\n",
    "Any of the features could have an impact on the Standard_yield. Let's begin with Ave_temps, the average temperature of the region, and its relationship to Standard_yield.\n",
    "\n",
    "Let's start with the basics: a scatter plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990be2e",
   "metadata": {},
   "source": [
    "**⚙️ Your task:**\n",
    "\n",
    " 1. Generate a scatter plot to visualise the relationship between `Ave_temps` and `Standard_yield`.\n",
    " 2. Reflect on the scatter plot. Does it suggest a linear relationship, or is the story more complex?\n",
    "\n",
    "**Note:**\n",
    "- Use `matplotlib` to create the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfd17a-019b-443a-8b43-de5428ea47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e717c-d9ca-41fd-8610-d990a4e181bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code to draw a scatter plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c62e2-a722-4ed0-8f81-a8773c231987",
   "metadata": {},
   "source": [
    "Now, let's write a function to calculate the Pearson correlation coefficient.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `get_correlation` that:\n",
    "1. Takes a DataFrame and the names of the columns we want to determine the correlation for as parameters (`Ave_temps` and `Standard_yield`).\n",
    "2. Calculates the Pearson correlation coefficient between these two columns to quantify their linear relationship.\n",
    "4. Returns the Pearson correlation coefficient.\n",
    "\n",
    "**Note:**\n",
    "- Use `scipy` to calculate the Pearson correlation coefficient.\n",
    "- Ensure your function returns the Pearson correlation coefficient as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e239f40-8324-46a5-aefd-b3c1a382591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86896dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def get_correlation(df, col1, col2):\n",
    "    \n",
    "    # Add code to calculate and return the correlation coefficient\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4474e",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = get_correlation(dataset,'Ave_temps','Standard_yield')\n",
    "print(\"Pearson correlation coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae64880",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "Correlation: 0.006785950289020164\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78795c87",
   "metadata": {},
   "source": [
    "What do you notice about the scatter plot and the dispersion of data points? It's essential to visualise our data first; if the data doesn't follow a linear pattern, then a linear regression model may fail to accurately capture the underlying relationship. The correlation also seems extremely low. What does this tell us?\n",
    "\n",
    "Let's write down some of our observations:\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372ccb9",
   "metadata": {},
   "source": [
    "## Challenge 2: A breath of fresh data – Pollution as a predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c4cc1",
   "metadata": {},
   "source": [
    "It's time to shift our gaze from the warmth of the sun to the haze of pollution. Could the levels of pollution, a concern for farmers and environmentalists alike, be an indicator of our yields?\n",
    "\n",
    "Let's begin by fitting a simple linear regression model to try and capture the linear relationship between these columns.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `fit_linear_regression_model` that:\n",
    "1. Takes in a DataFrame and the names of the `Pollution_level` and `Standard_yield` columns.\n",
    "2. Fits a linear regression model to the data.\n",
    "3. Returns the model, the model predictions, and the actual y-values.\n",
    "\n",
    "**Note:**\n",
    "- Use `LinearRegression` from `sklearn` to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe797b89-06c2-42a8-b637-1366fdd6968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def fit_linear_regression_model(df, pollution_col, yield_col):\n",
    "    \n",
    "    # Add code to fit the linear regression model and return the model, predictions, y-values.\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39424119",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predictions, y_values = fit_linear_regression_model(dataset, 'Pollution_level', 'Standard_yield')\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Actual Y-Values: {y_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a7f56",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "- Model: an instance of the LinearRegression class.\n",
    "- Predictions: a NumPy array of predicted values.\n",
    "- y: a Pandas Series with the actual target values used for training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22dcac-333a-453c-a89e-06bdeaf52595",
   "metadata": {},
   "source": [
    "Linear regression models only work well if our data are in fact linear. So, let's create a scatter plot to visualise the relationship between pollution and crop yields. In addition to this, let's use the predictions from the model we fit to add the line of best fit to our scatter plot.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Generate a scatter plot to visualise the effect that pollution has on standard yield.\n",
    "2. Draw the line of best fit.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "You can use this line of code to draw the regression line on the plot:\n",
    "`plt.plot(X, predictions, color='red', label='Regression line')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90161e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['Pollution_level']]\n",
    "y = dataset['Standard_yield']\n",
    "\n",
    "# Add code to draw the scatter plot and the regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f356b-c869-45f3-b263-dce583b55d7c",
   "metadata": {},
   "source": [
    "Now, use the get_correlation() function that we defined earlier to test the correlation between `Pollution_level` and `Standard_yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d826e9-ef70-4755-94d3-b0ace84c3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pollution_correlation = get_correlation(dataset,'Pollution_level','Standard_yield')\n",
    "print(\"Pearson correlation coefficient:\", Pollution_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedea5ad-301e-44c1-8f15-f82fbd496e96",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "Correlation: -0.2857609646210543\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0636d-3185-4462-885f-e57e8690e020",
   "metadata": {},
   "source": [
    "Reflect on the difference between this plot and correlation and the previous one with the average temperature. Is the relationship between pollution and yield more linear?\n",
    "\n",
    "✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1845f",
   "metadata": {},
   "source": [
    "\n",
    "We can also gain a better understanding of our model by examining the slope and intercept.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `get_slope_intercept` that:\n",
    "1. Inputs the `model` we fitted and calculates the slope and intercept of the line of best fit.\n",
    "2. Returns the slope and intercept as a tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e8020-4008-4787-8046-9924d3ca256f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def get_slope_intercept(model):\n",
    "    \n",
    "    #Add code to calculate and return the slope and intercept\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade93ef-6595-4c07-87d4-0f0e9cfecdfa",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2c04f-5c56-478a-b30e-50be0de2adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = get_slope_intercept(model)\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b1e0a-61da-4ca1-a4cd-46f44cb7e122",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Slope: -0.1427617720986604\n",
    "Intercept: 0.5662684415393379\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee5e4d-90af-445a-a8e0-5e50ca62657e",
   "metadata": {},
   "source": [
    "\n",
    "What does the slope tell us about the strength of the relationship between pollution and yield? Also, what can we learn from the y-intercept?\n",
    "\n",
    "  - ✍️ Your notes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d8de9",
   "metadata": {},
   "source": [
    "## Challenge 3: The haze clears: Evaluating pollution's predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc62613",
   "metadata": {},
   "source": [
    "When we look at the scatterplots of `Standard_yield` with `Ave_temps` and `Pollution_level`, it appears that pollution level might have a more linear relationship.  This means that we could potentially use a simple linear regression model to make predictions about the yield of a farm based on its pollution level. However, before we do this we need to further assess the strength of the linear relationship between `Pollution_level` and `Standard_yield`. \n",
    "\n",
    "Let's assess our model's performance using R-squared, mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `calculate_evaluation_metrics` that:\n",
    "1. Takes the predictions and y-values from our fitted model as input.\n",
    "2. Calculates and returns the R-squared, mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) of the model's performance.\n",
    "\n",
    "**Note:**\n",
    "1. Calculate the model's performance metrics using the entire dataset.\n",
    "2. Return the evaluation metrics as a tuple in the order: R-squared, MAE, MSE, RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29474e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf26a4b-e238-4d7c-882b-78373700b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def calculate_evaluation_metrics(predictions, y_values):\n",
    "    \n",
    "   # Add code to calculate and return the r2, mae, mse and rmse\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448a869",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c214f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = calculate_evaluation_metrics(predictions, y_values)\n",
    "print(f\"Evaluation Metrics:\\nR-squared: {evaluation_metrics[0]}\\nMAE: {evaluation_metrics[1]}\\nMSE: {evaluation_metrics[2]}\\nRMSE: {evaluation_metrics[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ce1a4",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
    "\n",
    "```python\n",
    "R-squared: 0.08165932890115546\n",
    "MAE: 0.08554642090904992\n",
    "MSE: 0.011477732254034848\n",
    "RMSE: 0.10713417873878928\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fb144",
   "metadata": {},
   "source": [
    "Think about what these metrics tell us about our model's accuracy and reliability. Write down your observations:\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977babed",
   "metadata": {},
   "source": [
    "## Challenge 4: The dividing line – Train-test split in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9f6f7",
   "metadata": {},
   "source": [
    "As we delve deeper into the relationship between `Pollution_level` and `Standard_yield`, we must ensure our model is not merely memorising the data but truly understanding it. This brings us to the pivotal technique of train-test split.\n",
    "\n",
    "**The importance of train-test split**\n",
    "\n",
    "Imagine teaching a student for an exam by using the very questions that will appear on it. They might score perfectly, but does it mean they've truly learned? Similarly, a model might perform exceptionally on the data it was trained on, but the real test of knowledge comes from unseen data. This is where the train-test split comes in, allowing us to assess our model's generalisation capabilities by training on one subset of data and testing on another.\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Create a function named `data_train_test_split` that:\n",
    "1. Takes in the DataFrame and the two columns we want to model the relationship between (`Pollution_level` and `Standard_yield`).\n",
    "2. Separates it into features (`X`) based on `Pollution_level` and the target (`y`) based on `Standard_yield`.\n",
    "3. Splits the data into training and testing sets using an 80-20 split and sets `random_state = 42` for reproducibility.\n",
    "4.  Returns a tuple containing: `X_train` and `X_test`, which are DataFrames containing features for training and testing, respectively, along with `y_train` and `y_test`, which are Series representing subsets of the original DataFrame's target variable for training and testing.\n",
    "\n",
    "**Note:**\n",
    "- Use `train_test_split` from `sklearn.model_selection` to split the data.\n",
    "- Train a linear regression model on the training set using `LinearRegression` from `sklearn.linear_model`.\n",
    "- If the random state is not set to `42` the code will not be marked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f5f0c-7b9e-41f7-bb35-0985fb03bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4791a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def data_train_test_split(df, pollution_col, yield_col): \n",
    "   \n",
    "   # Add code to calculate and return the X_train, X_test, y_train and y_test\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcc079",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01448eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_train_test_split(dataset, 'Pollution_level', 'Standard_yield')\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd44b50",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "- X_train: DataFrame, subset of the original DataFrame's features for training.\n",
    "- X_test: DataFrame, subset of the original DataFrame's features for testing.\n",
    "- y_train: Series, subset of the original DataFrame's target variable for training.\n",
    "- y_test: Series, subset of the original DataFrame's target variable for testing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3c4bf",
   "metadata": {},
   "source": [
    "Now let's fit a linear regression model to the data.\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Create a function named `train_split_linear_regression_model()` that:\n",
    "1. Takes `X_train`, `X_test`, `y_train`, `y_test` as input (the results from the `data_train_test_split()` function).\n",
    "2. Trains a simple linear regression model on the training set.\n",
    "3. Uses the testing set to make predictions.\n",
    "4. Returns a tuple containing: the model, the predictions, and y_test (the actual y values in the testing set values) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd774c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ead6f-83bc-4d65-8acc-09f8f1237a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def train_split_linear_regression_model(X_train, X_test, y_train, y_test): \n",
    "    \n",
    "    # Add code to fit the linear regression model and return the model, predictions and y_test\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3e0e3-e878-40f1-b73c-fe0cf20f3e8b",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bee552-6e1b-44ff-a868-39875f50bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_model, predictions_test, y_test = train_split_linear_regression_model(X_train, X_test, y_train, y_test)\n",
    "print(f\"Train-Test Model: {train_test_model}\")\n",
    "print(f\"Test Predictions: {predictions_test}\")\n",
    "print(f\"Test Actual Y-Values: {y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b9b0a-780d-41f1-99ed-049c1c0d0277",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "- Model: an instance of the LinearRegression class.\n",
    "- Predictions: a NumPy array of predicted values.\n",
    "- y_test: a Pandas Series with the actual target values used for evaluating the model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e49d6-b478-4720-8970-cbecc2e243cf",
   "metadata": {},
   "source": [
    "Now, let's evaluate our train-test model by determining R-squared, MAE, MSE, and RMSE.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Use the `calculate_evaluation_metrics` function (defined in Challenge 3) to calculate the R-squared, MAE, MSE, and RMSE.\n",
    "2. The function should return a tuple containing the evaluation metrics (R-squared, MAE, MSE, and RMSE).\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Ensure to use the test set to calculate the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879fc5b-0ee7-47ee-8c0d-ee5f48a3c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code to calculate the R-squared, MAE, MSE, and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc94aa-1188-4119-80a1-4cc22876ffc0",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
    "\n",
    "```python\n",
    "R-squared: 0.08065722992150859\n",
    "MAE:  0.08794942119747501\n",
    "MSE: 0.012250634233355654\n",
    "RMSE: 0.11068258324305434\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6334b-86a5-4884-b652-4888990e7066",
   "metadata": {},
   "source": [
    "Reflect on the difference between these metrics and the metrics we obtained from the previous model (that was not split into training and testing sets). Why do you think the fit is worse now? And why should we choose the worst option? (Reflect on the course material if the answers to these questions are not clear.)\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0acaa",
   "metadata": {},
   "source": [
    "## Challenge 5: Diagnosing model fit through residual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f331f",
   "metadata": {},
   "source": [
    "From our analysis, it seems as though neither `Ave_temps` nor `Pollution_level` have a strong linear fit with `Standard_yield`. However, even if we had obtained good results from our evaluation metrics, there are still other crucial assumptions we need to verify to ensure our model is well-fitted. Residual analysis plays a pivotal role in diagnosing the fit of linear regression models, helping us understand whether the assumptions of linearity, independence, and homoscedasticity (constant variance) of residuals are met. \n",
    "\n",
    "If they are not met, can we confidently model this problem using the model? And why?\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f05442-9497-4dc5-89c9-a9326dc1ff23",
   "metadata": {},
   "source": [
    "First, let's create a histogram.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Calculate the residuals of our train test model (difference between `y_test` and `predictions_test`).\n",
    "2. Plot these residuals as a histogram to assess their distribution and identify any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d7f10-33d2-4046-b944-1c52cceb8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9a4f9-9c6e-45b7-8f72-81bf2f652e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - predictions_test # calculating the residuals\n",
    "\n",
    "#  Add code to create a histogram of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c8ca6-061d-457c-975e-74d6c6411a09",
   "metadata": {},
   "source": [
    "What does the histogram tell us about our data?\n",
    "\n",
    "1. Examine the shape of the histogram. What does the distribution of residuals tell us about the normality of the data? Consider whether the residuals appear to be symmetrically distributed around zero.\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "2. Compare the tails of the histogram to a normal distribution. Are there signs of heavy tails or skewness that could affect the reliability of the regression model's predictions?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "3. Assess the centering of the histogram around the zero line. How does this central tendency reflect on the bias of the model's predictions?\n",
    "\n",
    "    - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7030a-452b-44da-94e6-caafce0e56c8",
   "metadata": {},
   "source": [
    "Now, let's create a scatter plot of these residuals against the predicted values. \n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Create a scatter plot of the residuals against the predicted values – `predictions_test` should be on the x-axis and the `residuals` on the y-axis.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- A horizontal line can be added at y=0 to make it easier to see if the residuals are evenly distributed around zero by adding this line of code:\n",
    "`plt.axhline(y=0, color='r', linestyle='--')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a7294-a0d3-4215-aa84-7213d2272bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add code to create a scatter plot of residuals against the predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab89c4-a6b3-4b4d-aee8-d3a67dcdc46b",
   "metadata": {},
   "source": [
    "Reflect on what the scatter plot tells us about our data and the fit of the model:\n",
    "\n",
    "1. Analyse the scatter plot for any apparent patterns or structures in the data. What does this suggest about the appropriateness of the linear regression model for the dataset?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "2. Inspect the plot for signs of heteroscedasticity. How does the spread of residuals change as the predicted values increase? What might this imply about the constant variance assumption in linear regression?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "3. Identify whether the residuals are evenly scattered above and below the zero line across the range of predicted values. What can this tell us about the model's performance in terms of bias and prediction accuracy?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "4. Look for outliers or clusters of points that deviate significantly from the majority. How might these points influence the overall fit of the model?\n",
    "\n",
    "    - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effb39d-3dd0-43bb-b1d1-01872990df99",
   "metadata": {},
   "source": [
    "Our final task is to examine the mean and standard deviation of the residuals, which provide further insights into the model's performance.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `calculate_residuals_statistics` that:\n",
    "1. Uses the `predictions_test` and `y_test` (obtained from Challenge 4) to calculate the residuals.\n",
    "2. Calculates the mean and standard deviation of the residuals.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Use `numpy` for the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd7922-1acc-44ae-932b-33187705678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ec84e-91f1-49fe-88cb-e1039d97527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def calculate_residuals_statistics(predictions, y_test):\n",
    "\n",
    "    # Add code to calculate and return the mean_residual and std_residual\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b2721-3ffd-4451-bce6-95c2ea6f676b",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9911-9425-476d-8753-f6c191fdb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_residual, std_residual = calculate_residuals_statistics(predictions_test, y_test)\n",
    "print(f\"Mean: {mean_residual}\\nStandard deviation: {std_residual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ebbe0-ddf0-41ff-a21d-8e195a9e18e7",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "Mean: 0.0058580231923217015\n",
    "Standard deviation: 0.11052745268770957\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d791017-94db-4400-8489-16c2b50a2dee",
   "metadata": {},
   "source": [
    "What does the mean of the residuals tell us about the bias in our predictions? How does a mean close to zero reflect on our model's accuracy?\n",
    "\n",
    "- ✍️ Your notes here\n",
    "\n",
    "What does the standard deviation of the residuals indicate about the variability of our predictions? Why is it important for this value to be relatively low?\n",
    "\n",
    " - ✍️ Your notes here\n",
    "\n",
    "What are the potential consequences of a high standard deviation of residuals on the reliability of the model's predictions? How might this affect our confidence in the model's estimates?\n",
    "\n",
    "- ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c96884-c710-49dd-a6ee-6f939420f661",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Linear regression, for all its strengths, assumes a straightforward relationship between the predictor and the outcome. Yet, the natural world seldom adheres to such simplicity. Factors influencing crop yields in Maji Ndogo – be it temperature, rainfall, or pollution – interact in complex, often nonlinear ways. Our initial model with `Ave_temps` hinted at this complexity, suggesting that the effect of the average temperature on yields might follow a more intricate pattern than a straight line can depict (or no pattern at all).\n",
    "\n",
    "Our yield also depends on more than just the pollution or the temperature; it depends on many of the factors – we could see that from our EDA. We also saw that not all crops are affected equally by pollution or temperature, so we could simplify our model if we remove the influence of the different crops. Once your submission is done, as a challenge to yourself, try to split the data again by crop type (with a loop) and use the functions you created to loop over all of the crop types and print out your metrics.\n",
    "\n",
    "Compare them, and discuss your results with your colleagues. Is there a crop type that is affected by pollution more than other crop types?\n",
    "\n",
    "As we dive deeper into regression, it's crucial to remember that with each model comes a new perspective. Just as a farmer selects the tool that best suits the task at hand, so we must choose our models with intention and insight. Exploring beyond linear regression opens up new vistas of understanding, allowing us to capture the richness of relationships within our data.\n",
    "\n",
    "Countless stories await us in the fields of Maji Ndogo, and beyond. It's up to us to uncover them – with curiosity as our guide and an ever-expanding array of models at our disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
